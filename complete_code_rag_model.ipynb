{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#### Note:\n",
        "##### - Furthermore, we can build a real-time chatbot with a UI using React. As mentioned, we cannot upload Python files or any other file types for UI development. However, such a chatbot can be easily hosted anywhere. For this, we need to collect a substantial dataset to improve it further. Given the short timeframe of 2–3 days, building it quickly in today’s competitive world is a bit challenging. However, I assure you that with more time, I can create a magnificent UI-based chatbot—not just for an assignment, but also to enhance my resume.\n",
        "\n",
        "##### In case I fail to reach relevance, please check out my GitHub for examples of fully deployed UI chatbots.\n",
        "\n",
        "---\n",
        "\n",
        "Let me know if you’d like further revisions!"
      ],
      "metadata": {
        "id": "uV7K9QDwQdxw"
      },
      "id": "uV7K9QDwQdxw"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#QA Bot of **Yardstick** Using RAG\n"
      ],
      "metadata": {
        "id": "_jj4mTRZucBJ"
      },
      "id": "_jj4mTRZucBJ"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### STEP 1"
      ],
      "metadata": {
        "id": "ETGfSM0yvLUl"
      },
      "id": "ETGfSM0yvLUl"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### setuping data from pdf to vector database in PINECONE"
      ],
      "metadata": {
        "id": "MiT2LR6C0cr1"
      },
      "id": "MiT2LR6C0cr1"
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install PyPDF2 openai pinecone-client\n"
      ],
      "metadata": {
        "id": "4xjZxsvc0khA"
      },
      "id": "4xjZxsvc0khA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# extracting the text data from pdf(local database)\n",
        "\n",
        "from PyPDF2 import PdfReader\n",
        "\n",
        "\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "  reader = PdfReader(pdf_path)\n",
        "  text = \"\"\n",
        "  for page in reader.pages:\n",
        "    text += page.extract_text()\n",
        "  return text\n",
        "\n",
        "pfd_txt = extract_text_from_pdf(\"/content/About Yardstick.pdf\")\n"
      ],
      "metadata": {
        "id": "H__q_b4D1VXh"
      },
      "id": "H__q_b4D1VXh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pfd_txt[:200]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "z2cS32se2PxS",
        "outputId": "1bc4e721-da96-42c1-97b4-815af7c1e51e"
      },
      "id": "z2cS32se2PxS",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'41 Essential Machine \\nLearning Interview \\nQuestions\\nwww.springboard.com\\n18 mins readM\\nachine learning interview questions are an integral part \\nof the data science interview and the path to becoming a'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# splitting the text into smaller chunks using the RecursiveCharacterTextSplitter\n",
        "\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "def vector_txt(txt, chunk_size=1000, chunk_overlap=200):\n",
        "  txt_splitter = RecursiveCharacterTextSplitter(\n",
        "      chunk_size=chunk_size,\n",
        "      chunk_overlap=chunk_overlap,\n",
        "      # length_function=len\n",
        "  )\n",
        "\n",
        "  texts = txt_splitter.split_text(txt)\n",
        "  return texts\n",
        "\n",
        "vec = vector_txt(pfd_txt)\n",
        "print(f'Number of chunks = {len(vec)}')\n",
        "print(f\"First chunk:\\n{vec[0]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SgNM4rQb2Sk4",
        "outputId": "7dad4ebc-89de-453e-8ee9-c5540244a6cd"
      },
      "id": "SgNM4rQb2Sk4",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of chunks = 6\n",
            "First chunk:\n",
            "About\n",
            "Yardstick\n",
            "Who\n",
            "and\n",
            "Why\n",
            "we\n",
            "are?\n",
            "Yardstick's\n",
            "vision\n",
            "is\n",
            "to\n",
            "make\n",
            "learning\n",
            "enriching\n",
            "and\n",
            "joyful\n",
            "experience.\n",
            "Yardstick\n",
            "designs\n",
            "and\n",
            "implements\n",
            "learning\n",
            "programs\n",
            "for\n",
            "children,\n",
            "engaging\n",
            "their \n",
            "keen,\n",
            "inquisitive\n",
            "and\n",
            "imaginative\n",
            "minds\n",
            "via\n",
            "holistic\n",
            "experiential\n",
            "learning\n",
            "modules.\n",
            "Yardstick\n",
            "provides\n",
            "specific\n",
            "services\n",
            "to\n",
            "all\n",
            "the\n",
            "stakeholders\n",
            "in\n",
            "a\n",
            "child’ s\n",
            "education \n",
            "–\n",
            "from\n",
            "parents,\n",
            "teachers\n",
            "and\n",
            "administrators\n",
            "to\n",
            "the\n",
            "students.\n",
            "Our\n",
            "activity-based \n",
            "curricula\n",
            "mapped\n",
            "to\n",
            "the\n",
            "syllabus\n",
            "encourage\n",
            "children\n",
            "to\n",
            "understand,\n",
            "appreciate \n",
            "and\n",
            "apply\n",
            "the\n",
            "subject\n",
            "being\n",
            "taught.\n",
            "Our\n",
            "team\n",
            "attempts\n",
            "to\n",
            "give\n",
            "personalized \n",
            "attention\n",
            "to\n",
            "every\n",
            "child.\n",
            "Yardstick\n",
            "offers\n",
            "outstanding,\n",
            "highly\n",
            "interactive,\n",
            "hands\n",
            "on\n",
            "curriculum\n",
            "that\n",
            "enables \n",
            "mastery\n",
            "of\n",
            "core\n",
            "concepts\n",
            "and\n",
            "skills\n",
            "for\n",
            "all\n",
            "kinds\n",
            "of\n",
            "minds.\n",
            "The\n",
            "curriculum\n",
            "focuses \n",
            "on\n",
            "unleashing\n",
            "creativity ,\n",
            "real\n",
            "life\n",
            "application,\n",
            "and\n",
            "understanding\n",
            "rather\n",
            "than \n",
            "memorizing,\n",
            "inquiry\n",
            "based\n",
            "hands\n",
            "on\n",
            "approach.\n",
            "How\n",
            "do\n",
            "we\n",
            "do\n",
            "it?\n",
            "Mission\n",
            "and\n",
            "Vision\n",
            "What\n",
            "do\n",
            "we\n",
            "dream? \n",
            "The\n",
            "Yardstick\n",
            "vision\n",
            "is\n",
            "to\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialing Pinecone and connecting to the index\n",
        "\n",
        "from pinecone import Pinecone, ServerlessSpec\n",
        "\n",
        "# Initialize Pinecone with the new API\n",
        "pc = Pinecone(\n",
        "    api_key=\"-\"\n",
        ")\n",
        "\n",
        "# Specify serverless environment\n",
        "spec = ServerlessSpec(\n",
        "    cloud=\"aws\",\n",
        "    region=\"us-east-1\"\n",
        ")\n",
        "\n",
        "# Create or connect to the index\n",
        "index_name = \"yardstick-qa\"\n",
        "if index_name not in pc.list_indexes().names():\n",
        "    pc.create_index(\n",
        "        name=index_name,\n",
        "        dimension=1536,\n",
        "        metric=\"cosine\",\n",
        "        spec=spec\n",
        "    )\n",
        "else:\n",
        "    print(f\"Index '{index_name}' already exists.\")\n",
        "\n",
        "index = pc.Index(index_name)\n"
      ],
      "metadata": {
        "id": "jOKtG4wG5uK0"
      },
      "id": "jOKtG4wG5uK0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# here OpenAI text-embedding-ada-002 model embeddings each text chunk and then upserts them into\n",
        "# the Pinecone index.\n",
        "\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "\n",
        "embedding = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
        "\n",
        "# Embed and upsert each chunk into Pinecone\n",
        "for i, text in enumerate(vec):\n",
        "    chunk_embedding = embedding.embed_query(text)\n",
        "    index.upsert([(f\"chunk-{i}\", chunk_embedding, {\"text\": text})])\n"
      ],
      "metadata": {
        "id": "A4OMP8oPLw7f"
      },
      "id": "A4OMP8oPLw7f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "retriver from pincone of stored data"
      ],
      "metadata": {
        "id": "msULuVGDGua1"
      },
      "id": "msULuVGDGua1"
    },
    {
      "cell_type": "code",
      "source": [
        "# retriving from the pinecone\n",
        "\n",
        "from langchain.vectorstores import Pinecone\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "\n",
        "retriever = Pinecone(index=index,\n",
        "    embedding=embedding.embed_query,\n",
        "                     text_key='text'\n",
        "\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-tbhUZUkEGVn",
        "outputId": "79c4d0b2-5f98-4af2-9737-cdace2441ce8"
      },
      "id": "-tbhUZUkEGVn",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_community/vectorstores/pinecone.py:68: UserWarning: Passing in `embedding` as a Callable is deprecated. Please pass in an Embeddings object instead.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step2"
      ],
      "metadata": {
        "id": "-BESdr8Cyg3K"
      },
      "id": "-BESdr8Cyg3K"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Building the RAG model using the costom dataset"
      ],
      "metadata": {
        "id": "spch6b3oylN3"
      },
      "id": "spch6b3oylN3"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "r82auLCIH0P0"
      },
      "id": "r82auLCIH0P0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "\n",
        "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.7)\n",
        "\n",
        "# Instead of directly passing 'retriever', use retriever.as_retriever()\n",
        "rag_model = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    retriever=retriever.as_retriever() # Call as_retriever() method\n",
        ")"
      ],
      "metadata": {
        "id": "DMH3V0Pr_WgR"
      },
      "id": "DMH3V0Pr_WgR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# testing the model"
      ],
      "metadata": {
        "id": "GBhVuZkSzEem"
      },
      "id": "GBhVuZkSzEem"
    },
    {
      "cell_type": "code",
      "source": [
        "ques = '''what is question number 1 in Essential Machine\n",
        "Learning Interview\n",
        "Questions'''\n",
        "ans = rag_model.run(ques)\n",
        "print(ans)\n"
      ],
      "metadata": {
        "id": "zUzwvC3XCk-6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "433517e6-95d8-4d74-8b84-b82288d1ef3f"
      },
      "id": "zUzwvC3XCk-6",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question number 1 in the Essential Machine Learning Interview Questions is: \"What’s the trade-off between bias and variance?\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4mBp7H3mEobo"
      },
      "id": "4mBp7H3mEobo",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jEUbudsA0Gt4"
      },
      "id": "jEUbudsA0Gt4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OxDb5PJ90Gpv"
      },
      "id": "OxDb5PJ90Gpv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bot just using normal model(GPT) without RAG"
      ],
      "metadata": {
        "id": "hNoyGYe10HZk"
      },
      "id": "hNoyGYe10HZk"
    },
    {
      "cell_type": "markdown",
      "source": [
        "The responce is complete different from above method"
      ],
      "metadata": {
        "id": "5Vqkj3n40eW2"
      },
      "id": "5Vqkj3n40eW2"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "49b3bbee-de63-41c6-8c24-44bbdeb4ecaf",
      "metadata": {
        "id": "49b3bbee-de63-41c6-8c24-44bbdeb4ecaf"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from langchain.chat_models import ChatOpenAI"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install -U langchain-openai\n",
        "# !pip install langchain-community"
      ],
      "metadata": {
        "id": "lfD7EyP-zpEG"
      },
      "id": "lfD7EyP-zpEG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = '--'\n",
        "chat = ChatOpenAI(\n",
        "    openai_api_key=os.environ[\"OPENAI_API_KEY\"],\n",
        "    model='gpt-3.5-turbo'\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "pe3M87N2ief5"
      },
      "id": "pe3M87N2ief5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.schema import (\n",
        "    AIMessage,\n",
        "    HumanMessage,\n",
        "    SystemMessage\n",
        ")\n"
      ],
      "metadata": {
        "id": "xFU4PV3Tj3YF"
      },
      "id": "xFU4PV3Tj3YF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mes = [\n",
        "    HumanMessage(content= '''what is question number 1 in Essential Machine\n",
        "Learning Interview\n",
        "Questions''')\n",
        "]"
      ],
      "metadata": {
        "id": "gMCtEMuej6lQ"
      },
      "id": "gMCtEMuej6lQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "res = chat(mes)\n",
        "print(res.content)"
      ],
      "metadata": {
        "id": "2Nj5O7qjz9fb"
      },
      "id": "2Nj5O7qjz9fb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ans = []\n",
        "ans.append(res.content)\n",
        "print(ans)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Hr-aWzGvmgI",
        "outputId": "db49b93d-f384-4866-f814-f36e7a7291c4"
      },
      "id": "2Hr-aWzGvmgI",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Yardstick is a technology and professional services company that specializes in assessment and credentialing solutions. They offer a range of services including exam development, psychometric analysis, test administration, and certification management. Yardstick works with a variety of industries and organizations to create customized assessment programs that meet their specific needs. They are known for their innovative approach to assessment and their commitment to providing reliable and valid results for their clients. Yardstick is based in Canada but serves clients around the world.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oTKPymDBGqjA"
      },
      "id": "oTKPymDBGqjA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7QMLBxZ1Gqfh"
      },
      "id": "7QMLBxZ1Gqfh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine Tunned Model developed by using Fastapi"
      ],
      "metadata": {
        "id": "LGqXUSXh5N1G"
      },
      "id": "LGqXUSXh5N1G"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Method 1 openAi CLI tool"
      ],
      "metadata": {
        "id": "V8PeJj2LM8Ba"
      },
      "id": "V8PeJj2LM8Ba"
    },
    {
      "cell_type": "code",
      "source": [
        "!openai tools fine_tunes.prepare_data -f/content/fine_tuned_dataset.jsonl"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Z_c3-yl751O",
        "outputId": "1f3d0815-9748-4b33-8c33-725ee8c6cb1c"
      },
      "id": "4Z_c3-yl751O",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Analyzing...\n",
            "\n",
            "- Your file contains 112 prompt-completion pairs\n",
            "- Based on your data it seems like you're trying to fine-tune a model for classification\n",
            "- For classification, we recommend you try one of the faster and cheaper models, such as `ada`\n",
            "- For classification, you can estimate the expected model performance by keeping a held out dataset, which is not used for training\n",
            "- There are 98 duplicated prompt-completion sets. These are rows: [14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111]\n",
            "- All prompts end with suffix `\\n\\n###\\n\\n`\n",
            "- The completion should start with a whitespace character (` `). This tends to produce better results due to the tokenization we use. See https://platform.openai.com/docs/guides/fine-tuning/preparing-your-dataset for more details\n",
            "\n",
            "Based on the analysis we will perform the following actions:\n",
            "- [Recommended] Remove 98 duplicate rows [Y/n]: n\n",
            "- [Recommended] Add a whitespace character to the beginning of the completion [Y/n]: y\n",
            "- [Recommended] Would you like to split into training and validation set? [Y/n]: n\n",
            "\n",
            "\n",
            "Your data will be written to a new JSONL file. Proceed [Y/n]: y\n",
            "\n",
            "Wrote modified file to `/content/fine_tuned_dataset_prepared.jsonl`\n",
            "Feel free to take a look!\n",
            "\n",
            "Now use that file when fine-tuning:\n",
            "> openai api fine_tunes.create -t \"/content/fine_tuned_dataset_prepared.jsonl\"\n",
            "\n",
            "After you’ve fine-tuned a model, remember that your prompt has to end with the indicator string `\\n\\n###\\n\\n` for the model to start generating completions, rather than continuing with the prompt. Make sure to include `stop=[\"\\n\"]` so that the generated texts ends at the expected place.\n",
            "Once your model starts training, it'll approximately take 5.02 minutes to train a `curie` model, and less for `ada` and `babbage`. Queue will approximately take half an hour per job ahead of you.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!openai api fine_tunes.create -t \"/content/fine_tuned_dataset_prepared.jsonl\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rnuNRBikDW_u",
        "outputId": "91e06bff-0c0b-41bf-ca1e-b1b5ca997c5e"
      },
      "id": "rnuNRBikDW_u",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "usage: openai api [-h]\n",
            "                  {chat.completions.create,images.generate,images.edit,images.create_variation,audio.transcriptions.create,audio.translations.create,files.create,files.retrieve,files.delete,files.list,models.list,models.retrieve,models.delete,completions.create}\n",
            "                  ...\n",
            "openai api: error: argument {chat.completions.create,images.generate,images.edit,images.create_variation,audio.transcriptions.create,audio.translations.create,files.create,files.retrieve,files.delete,files.list,models.list,models.retrieve,models.delete,completions.create}: invalid choice: 'fine_tunes.create' (choose from 'chat.completions.create', 'images.generate', 'images.edit', 'images.create_variation', 'audio.transcriptions.create', 'audio.translations.create', 'files.create', 'files.retrieve', 'files.delete', 'files.list', 'models.list', 'models.retrieve', 'models.delete', 'completions.create')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install fastapi uvicorn openai langchain pinecone"
      ],
      "metadata": {
        "id": "xMEp7S3v5ruM"
      },
      "id": "xMEp7S3v5ruM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The above method of fine tuning using CLI method showd some api arguments error so we go for another steps\n",
        "The error are shown in the above"
      ],
      "metadata": {
        "id": "BzWHr53nNNJM"
      },
      "id": "BzWHr53nNNJM"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Method 2"
      ],
      "metadata": {
        "id": "czVpMAyaNrl7"
      },
      "id": "czVpMAyaNrl7"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Directly taking prepared dataset for Fine Tuning"
      ],
      "metadata": {
        "id": "jUc9WIFW7L7y"
      },
      "id": "jUc9WIFW7L7y"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using the Method of Contextual Prompt Engineering"
      ],
      "metadata": {
        "id": "xR-buxdq_m8v"
      },
      "id": "xR-buxdq_m8v"
    },
    {
      "cell_type": "code",
      "source": [
        "from PyPDF2 import PdfReader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from fastapi import FastAPI, HTTPException\n",
        "import openai\n",
        "from pinecone import Pinecone, ServerlessSpec\n",
        "\n",
        "from pydantic import BaseModel\n",
        "\n",
        "# Set API keys\n",
        "openai.api_key = \"=A\"\n",
        "pc = Pinecone(\n",
        "    api_key=\"-\"\n",
        ")\n",
        "\n",
        "# Specify serverless environment\n",
        "spec = ServerlessSpec(\n",
        "    cloud=\"aws\",\n",
        "    region=\"us-east-1\"\n",
        ")\n",
        "\n",
        "# Create or connect to the index\n",
        "index_name = \"yardstick-qa\"\n",
        "if index_name not in pc.list_indexes().names():\n",
        "    pc.create_index(\n",
        "        name=index_name,\n",
        "        dimension=1536,\n",
        "        metric=\"cosine\",\n",
        "        spec=spec\n",
        "    )\n",
        "else:\n",
        "    print(f\"Index '{index_name}' already exists.\")\n",
        "\n",
        "index = pc.Index(index_name)\n",
        "app = FastAPI()\n",
        "\n",
        "# Define the input model\n",
        "class QueryRequest(BaseModel):\n",
        "    query: str\n",
        "\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "\n",
        "def extract_text_from_pdfs(pdf_paths):\n",
        "    all_texts = []\n",
        "    for pdf_path in pdf_paths:\n",
        "        reader = PdfReader(pdf_path)\n",
        "        text = \"\"\n",
        "        for page in reader.pages:\n",
        "            text += page.extract_text()\n",
        "\n",
        "        # Splitting the text into smaller chunks using the RecursiveCharacterTextSplitter\n",
        "        txt_splitter = RecursiveCharacterTextSplitter(\n",
        "            chunk_size=1000,\n",
        "            chunk_overlap=200,\n",
        "            # length_function=len\n",
        "        )\n",
        "\n",
        "        texts = txt_splitter.split_text(text)\n",
        "        all_texts.extend(texts)\n",
        "\n",
        "    return all_texts\n",
        "\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "\n",
        "embedding = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
        "\n",
        "# Example usage\n",
        "pdf_paths = [\"/content/About Yardstick.pdf\"]\n",
        "all_texts = extract_text_from_pdfs(pdf_paths)\n",
        "\n",
        "# Embed and upsert each chunk into Pinecone\n",
        "for i, text in enumerate(all_texts):\n",
        "    chunk_embedding = embedding.embed_query(text)\n",
        "    index.upsert([(f\"chunk-{i}\", chunk_embedding, {\"text\": text})])\n",
        "\n",
        "from langchain.vectorstores import Pinecone\n",
        "\n",
        "retriever = Pinecone(\n",
        "    index=index,\n",
        "    embedding=embedding.embed_query,\n",
        "    text_key='text'\n",
        ")\n",
        "\n",
        "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.7)\n",
        "\n",
        "# Instead of directly passing 'retriever', use retriever.as_retriever()\n",
        "rag_model = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    retriever=retriever.as_retriever() # Call as_retriever() method\n",
        ")\n",
        "\n",
        "print(f'Number of chunks = {len(all_texts)}')\n",
        "# print(f\"First chunk:\\n{all_texts[0]}\")\n",
        "\n",
        "@app.get('/')\n",
        "def homePage():\n",
        "    return {'HomePage'}\n",
        "\n",
        "@app.post('/chat')\n",
        "def qa_chatbot(req: QueryRequest):\n",
        "    ques = req.query\n",
        "    if not ques:\n",
        "        raise HTTPException(status_code=400, detail='Query failed')\n",
        "\n",
        "    try:\n",
        "        answer = rag_model.run(ques)\n",
        "        return {\"query\": ques, \"answer\": answer}\n",
        "    except Exception as e:\n",
        "        raise HTTPException(status_code=500, detail=str(e))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vwv5rvj77DkF",
        "outputId": "3874c2de-3f89-4fb7-e514-29bbc4ea2dd8"
      },
      "id": "vwv5rvj77DkF",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index 'yardstick-qa' already exists.\n",
            "Number of chunks = 6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_community/vectorstores/pinecone.py:68: UserWarning: Passing in `embedding` as a Callable is deprecated. Please pass in an Embeddings object instead.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "RqGVUUMvNxkD"
      },
      "id": "RqGVUUMvNxkD"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RhdUofxmFhGe"
      },
      "id": "RhdUofxmFhGe",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}